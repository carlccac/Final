{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling and cleaning:\n",
    "\n",
    "Two data sets will be created; one with candidate financial data from Propublica, the other with the representatives for each congressional district taken from Wikipedia. The Propublica data set will be updated to represent winners and losers as per the wikipedia data set. A few records from the Propublica data set will need to be updated by hand as there were several duplicate records and two that did not match up from the Wikipedia data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Wrangle Propublica API Data\n",
    "\n",
    "Two functions were created that would be applied over a list of all 50 states to compile finacial data for all candidates in the 2020 congressional election. The function st() makes an initial request to Propublica to get a list of all the candidates who ran. The function then iterates over that list, making repeated requests to Propublica for each individual candidate's financial data. A pandas data frame is returned. An additional function, er(), is utiized by st() to catch any errors encountered. If er() encounters an error, the function calls itself again, recursively, or, returns the request if no error is encountered.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json; import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er(url, head): # error catch, recursive\n",
    "    global r; r = requests.get(url, headers = head)\n",
    "    if r.status_code != 200: er(url, head) \n",
    "    if r.text[11:13] != 'OK': er(url, head)\n",
    "    return r\n",
    "\n",
    "\n",
    "def st(state): #get list of candidates for each state and then each candidates' info.\n",
    "    df = pd.DataFrame(columns=['st', 'dist', 'name', 'id', 'party', 'total', 'pac', 'win'])\n",
    "    u = 'https://api.propublica.org/campaign-finance/v1/2020'\n",
    "    h = {'X-API-Key': '1-800-API-KEYS-R-US'} # dummy api key\n",
    "    cans = er(u+'/races/%s/house.json' %state, h).json()['results'] # get list of candidates who ran in each state\n",
    "    \n",
    "    for i in range(len(cans)): # enter list into data frame\n",
    "        df = df.append({'st': state,\n",
    "                        'name': cans[i]['candidate']['name'].lower(), \n",
    "                        'id': cans[i]['candidate']['id'], \n",
    "                        'party':cans[i]['candidate']['party'],},ignore_index = True)\n",
    "        \n",
    "    for j in range(len(df)):\n",
    "        can_data = er(u+'/candidates/%s.json' %(df.iloc[j,3]), h).json() # get individual candidate info\n",
    "        district = can_data['results'][0]['district']\n",
    "        df.iat[j,1] = 98 if district == None else 99 if not district[16:18].isnumeric() else int(district[16:18])\n",
    "        df.iat[j,5] = int(can_data['results'][0]['total_contributions'])\n",
    "        df.iat[j,6] = int(can_data['results'][0]['total_from_pacs'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States that are 'at-large' have no districts and the request to Propublica returns 'None' for the district; these were marked as '98' in the data frame. Also, if there is an error in the candidate's district, '99' was entered into the data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', \n",
    "    'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', \n",
    "    'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all 50 states, call st() for each state, concatenate to df1\n",
    "df1 = st(states[0])\n",
    "for i in range(1, len(states)): df1 = pd.concat([df1, st(states[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('propub_csv_final', index = None, header=True) #save CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Wrangle Wikipedia Table\n",
    "\n",
    "The function wiki() scrapes a Wikipedia page listing all 435 congressional districts and their representatives and returns a data frame. The function abbreviate() changes each state's full name to its abbreviation so as to match the Propublica data. Also, the function spanish() removes any accented characters so as to better match Propublica's data.\n",
    "\n",
    "As some candidates have complicated names, such as George Joseph \"Mike\" Kelly Jr. from PA 16, a search was conducted on the FEC website for each candidate's FEC ID. This will aid in matching the Propublica candidates to the Wikipedia candidates as the names are not perfect matches. Three functions were utilized. url_nm() parses the candidate's name and returns an appropriate string for the FEC website's search feature. ids() scrapes the results of the page returned from the FEC name search and returns the ID. wiki_id() iterates over each name in the Wikipedia data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate(x): #converts state's full name to abbreviation\n",
    "    abv = {'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO', \n",
    "     'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID', \n",
    "     'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', \n",
    "     'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "     'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', \n",
    "     'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', \n",
    "     'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC', \n",
    "     'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', \n",
    "     'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'}\n",
    "    a = x.split(); b = len(a)\n",
    "    return abv[a[0]] if b == 2 else abv[' '.join([a[0], a[1]])]\n",
    "\n",
    "def spanish(x): #converts accented characters to non-accented\n",
    "    sp = {'á': 'a', 'é': 'e', 'í': 'i', 'ú': 'u'}\n",
    "    return ''.join(sp[i] if i in sp.keys() else i for i in list(x))\n",
    "        \n",
    "\n",
    "def wiki(): #scrape Wikipedia table of congressional districts' representatives\n",
    "    df = pd.DataFrame(columns=['st', 'dist', 'st/dist','name','party', 'id'])\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_current_members_of_the_United_States_House_of_Representatives'\n",
    "    s = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    rows = s.find('table',{'class': 'wikitable sortable', 'id':'votingmembers'}).tbody.find_all('tr')\n",
    "    \n",
    "    for i in range(1, len(rows)):\n",
    "        tds = rows[i].find_all('td')\n",
    "        if len(tds) == 9:\n",
    "            values = [tds[0].text.replace('\\n', ''), spanish(tds[1].text.replace('\\n', '').lower()), \n",
    "                      tds[3].text.replace('\\n', '')]\n",
    "        else: values = [tds[0].text.replace('\\n', ''), 'vacant', 'vacant']\n",
    "        df = df.append(pd.Series(values, index=['st/dist','name','party']), ignore_index=True)\n",
    "        \n",
    "        df['dist'] = df['st/dist'].map(lambda x: int(x.split()[-1]) if x.split()[-1].isnumeric() else 98) #'at-large'\n",
    "        df['st'] = df['st/dist'].map(abbreviate)\n",
    "        df['party'] = df['party'].map(lambda x: x[0:3].upper())\n",
    "\n",
    "    return df\n",
    "\n",
    "def url_nm(name): return ''.join([i +'+' for i in name.split()]) #create FEC url name search string\n",
    "\n",
    "def ids(x): # FEC search request to get candidate's ID\n",
    "    result = BeautifulSoup(requests.get('https://www.fec.gov/search/?query=%s&type=candidates' %x).text,\n",
    "                         'html.parser').find_all(class_='post__path t-small t-sans')\n",
    "    return result[0].text.replace('<div>', '').split('/')[3] if result !=[] else 'no result'\n",
    "\n",
    "def wiki_id(x): #iterate over df apply FEC search for each district's winner\n",
    "    for i in range(len(x)):\n",
    "        x.iat[i,5] = ids(url_nm(x.iat[i,3]))\n",
    "    return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " wk1 = wiki_id(wiki()) #create wiki data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " wk1.to_csv('wiki_fec_csv', index = None, header=True) #save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pd.read_csv('/.../Desktop/propub_csv_final') # Propublica data frame\n",
    "wk = pd.read_csv('/.../Desktop/wiki_fec_csv')     # Wikipedia data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to go through the list of Propublica candidates and determine who won and who lost. The following for loop matches Propublica candidates to winning candidates from Wikipedia. For each Propublica candidate, look up their state and district in the Wikipedia table, a winner will match on the name or the FEC ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over every propublica candidate, did they win or lose?\n",
    "for i in range(len(pp)):\n",
    "    match = wk[(wk['st']==pp.iloc[i,0]) & (wk['dist']==pp.iloc[i,1])] #find candidate's state/district in the wiki df\n",
    "    if match.empty: pp.iat[i, 7] = False #accounts for incorrect districts in Propublica data\n",
    "    elif match['name'].values == 'vacant': pp.iat[i, 7] = False # there are currently 5 vacant seats\n",
    "    else: \n",
    "        pp.iat[i, 7]= ((pp.iloc[i,2].split(',')[0] == match['name'].values[0].split(' ')[-1]) \n",
    "        or (pp.iloc[i,3] == match.id.values[0])) # match on name or FEC ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia says there are 435 members of congress. How many do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n"
     ]
    }
   ],
   "source": [
    "print(len(pp[pp['win']==1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have too many, something is off. Maybe there are duplicate winners? We can filter the data frame to show all winners, then group by state and distict, counting the number of winners. There should be one and only one for each district. We can filter again returning any districts with a winner count of 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_dup = pp[pp['win']==1.0].groupby(['st', 'dist'], as_index = False).agg({'win':['count']})\n",
    "pp_dup = pp_dup[pp_dup['win']['count']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     st dist   win\n",
      "             count\n",
      "45   CA   25     2\n",
      "56   CA   36     2\n",
      "62   CA   42     2\n",
      "70   CA   50     2\n",
      "130  IA    2     2\n",
      "175  LA    5     2\n",
      "191  MD    6     2\n",
      "192  MD    7     2\n",
      "266  NV    3     2\n",
      "331  PA   12     2\n"
     ]
    }
   ],
   "source": [
    "print(pp_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are duplicates. Let's print out the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     st  dist             name         id party    total     pac  win\n",
      "270  CA    25     garcia, mike  H0CA25162   REP   358113    8893  1.0\n",
      "369  CA    25  garcia, michael  H0CA25105   REP  9348402  897272  1.0\n",
      "     st  dist            name         id party    total     pac  win\n",
      "173  CA    36      ruiz, raul  H0CA36177   REP    12039       0  1.0\n",
      "504  CA    36  ruiz, raul dr.  H2CA36439   DEM  2428672  968325  1.0\n",
      "     st  dist          name         id party    total     pac  win\n",
      "180  CA    42  calvert, ken  H0CA42209   REP        0       0  1.0\n",
      "505  CA    42  calvert, ken  H2CA37023   REP  1386747  611550  1.0\n",
      "     st  dist           name         id party     total     pac  win\n",
      "278  CA    50  issa, darrell  H0CA50178   REP  10196220  414060  1.0\n",
      "300  CA    50  issa, darrell  H0CA48024   REP         0       0  1.0\n",
      "      st  dist                            name         id party    total     pac  win\n",
      "1254  IA     2  miller-meeks, mariannette jane  H0IA02180   REP   748422  110770  1.0\n",
      "1264  IA     2  miller-meeks, mariannette jane  H8IA02043   REP  1585688  367312  1.0\n",
      "      st  dist                 name         id party    total     pac  win\n",
      "1365  LA     5        letlow, julia  H2LA05126   REP        0       0  1.0\n",
      "1384  LA     5  letlow, luke joshua  H0LA05120   REP  1395022  190150  1.0\n",
      "      st  dist          name         id party    total  pac  win\n",
      "1406  MD     6  trone, david  H8MD06168   DEM  2703087    0  1.0\n",
      "1437  MD     6  trone, david  H6MD08549   DEM  2704019    0  1.0\n",
      "      st  dist           name         id party    total     pac  win\n",
      "1411  MD     7  mfume, kweisi  H0MD07114   DEM   457321   78495  1.0\n",
      "1493  MD     7  mfume, kweisi  H6MD07020   DEM  1006454  368319  1.0\n",
      "      st  dist        name         id party    total      pac  win\n",
      "1872  NV     3  lee, susie  H8NV03200   DEM        0        0  1.0\n",
      "1904  NV     3  lee, susie  H6NV04020   DEM  4137510  1286408  1.0\n",
      "      st  dist          name         id party    total     pac  win\n",
      "2663  PA    12  keller, fred  H0PA12181   REP  1512978  445712  1.0\n",
      "2673  PA    12  keller, fred  H9PA12018   REP        0       0  1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pp_dup)):\n",
    "    print(pp[(pp['st']==ppd.iloc[i,0]) & (pp['dist']==ppd.iloc[i,1]) & (pp['win']==1.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the records...\n",
    "\n",
    "Duplicates with $0 donations:\n",
    "\n",
    "    2673  PA    12  keller, fred  H9PA12018\n",
    "    1872  NV     3  lee, susie  H8NV03200 \n",
    "    300  CA    50  issa, darrell  H0CA48024\n",
    "    180  CA    42  calvert, ken  H0CA42209\n",
    "\n",
    "Opposing candidate with nearly identical name:\n",
    "\n",
    "    173  CA    36      ruiz, raul  H0CA36177\n",
    "    \n",
    "Julia Letlow won special election to replace deceased husband:\n",
    "\n",
    "    1365  LA     5        letlow, julia  H2LA05126\n",
    "    \n",
    "No record found on FEC website for:\n",
    "\n",
    "    1411  MD     7  mfume, kweisi  H0MD07114\n",
    "    1406  MD     6  trone, david  H8MD06168\n",
    "    1035  IA     2  miller-meeks, mariannette jane  H0IA02180\n",
    "    289  CA    25     garcia, mike  H0CA25162\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "438 minus 10 duplicates = 428 members. Now we are short.\n",
    "\n",
    "\n",
    "Is there a winner for every district listed on Wikipedia? For each state and district of the wikipedia data frame is there a candidate in the propublica data frame with the same state and district who won?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk['match'] = wk.apply(lambda i: \n",
    "                         not (pp[(pp['st']==i.st) & (pp['dist']==i.dist) & (pp['win'] == 1.0)]).empty, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     st  dist       st/dist              name party         id  match\n",
      "106  FL    20    Florida 20            vacant   VAC  no result  False\n",
      "174  LA     2   Louisiana 2            vacant   VAC  no result  False\n",
      "254  NM     1  New Mexico 1            vacant   VAC  no result  False\n",
      "308  OH    11       Ohio 11            vacant   VAC  no result  False\n",
      "366  TX     6       Texas 6            vacant   VAC  no result  False\n",
      "384  TX    24      Texas 24    beth van duyne   REP  no result  False\n",
      "394  TX    34      Texas 34  filemon vela jr.   DEM  no result  False\n"
     ]
    }
   ],
   "source": [
    "print(wk[wk['match']==False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "438 minus 10 duplicates + 5 vacancies + 2 missing = 435\n",
    "\n",
    "The CSV file was edited by hand to correct for the 10 duplicate results and 2 missing candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "propub_winners_csv = pp.sort_values((['st', 'dist']), ascending=True)\n",
    "propub_winners_csv.to_csv('propub_winners_csv', index = None, header=True) # save to csv for editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pd.read_csv('/.../Desktop/propub_winners_csv_edit.csv') # open edited version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n"
     ]
    }
   ],
   "source": [
    "print(len(win[win['win']==1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
